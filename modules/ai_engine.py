import streamlit as st
from groq import Groq
import google.generativeai as genai

# --- 2026 API KEYS ---
GROQ_API_KEY = "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"
GEMINI_API_KEY = "AIzaSyAEssaFWdLqI3ie8y3eiZBuw8NVdxRzYB0"

def get_ai_strategic_insight(df, tab_name, engine="groq", custom_prompt=None):
    try:
        data_summary = df.tail(20).to_string()
        system_msg = (
            "You are a Senior Strategic Analyst. "
            "Start every report with a 'Data Health Grade' (A, B, or C)."
        )
        
        user_msg = f"Data:\n{data_summary}\n\nTask: Analyze {tab_name}." if not custom_prompt else custom_prompt

        # --- ENGINE: GEMINI ---
        if engine == "gemini":
            try:
                genai.configure(api_key=GEMINI_API_KEY)
                model = genai.GenerativeModel('gemini-3-flash-preview')
                response = model.generate_content(f"{system_msg}\n\n{user_msg}")
                # We add a clear label at the end
                return response.text + "\n\n---\n*Report generated by: **Google Gemini 3***"
            except Exception as e:
                # If Gemini fails, we switch to Groq
                st.toast("Gemini 404/Error: Falling back to Groq...", icon="üîÑ")
                engine = "groq"

        # --- ENGINE: GROQ ---
        if engine == "groq":
            client = Groq(api_key=GROQ_API_KEY)
            response = client.chat.completions.create(
                model="llama-3.3-70b-versatile",
                messages=[
                    {"role": "system", "content": system_msg},
                    {"role": "user", "content": user_msg}
                ]
            )
            # We add a clear label at the end
            return response.choices[0].message.content + "\n\n---\n*Report generated by: **Groq (Llama 3.3)***"

    except Exception as e:
        return f"‚ùå Critical Engine Error: {str(e)}"
